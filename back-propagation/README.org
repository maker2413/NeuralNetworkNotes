#+TITLE: Micrograd
#+PROPERTY: header-args:jupyter-python :session micrograd
#+PROPERTY: header-args:jupyter-python+ :async yes
#+PROPERTY: header-args:jupyter-python+ :tangle micrograd.py
#+PROPERTY: header-args:jupyter-python+ :results raw drawer
#+PROPERTY: header-args:jupyter-python+ :shebang "#!/usr/bin/env python"

This is the second part of my notes on neural networks. The first part was on
[[../gradients/][gradients]] and that part should be viewed first if this neural networks are new
to you.

This is simply my reimplementation of [[https://github.com/karpathy/micrograd][micrograd]] and exists only for me to learn
more about neural network technologies. I worked on this while following along
with this video: https://www.youtube.com/watch?v=VMj-3S1tku0.

#+begin_src jupyter-python :results none :exports none
  # This file was generated from the code blocks in ./README.org.
#+end_src

In the previous section of this repo we built out a couple of functions to graph
our equations and we will be reusing these functions in this example. Let's
start by importing some python libraries:
#+begin_src jupyter-python :results none
  #!/usr/bin/env python
  # Same libraries as before:
  import math
  from graphviz import Digraph
#+end_src

Now let's create those graphing functions previously mentioned:
#+begin_src jupyter-python :results none
  # Same trace function as before:
  def trace(root):
      # builds a set of all nodes and edges in a graph
      nodes, edges = set(), set()
      def build(v):
          if v not in nodes:
              nodes.add(v)
              for child in v._prev:
                  edges.add((child, v))
                  build(child)
      build(root)
      return nodes, edges

  # Same draw_dot function as before:
  def draw_dot(root):
      dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right

      nodes, edges = trace(root)
      for n in nodes:
          uid = str(id(n))
          # for any value in the graph, create a rectangular ('record') node for it
          dot.node(
              name = uid, label = "{ %s | data %.4f | grad %.4f }" % (
                  n.label,
                  n.data,
                  n.grad
              ),
              shape='record'
          )
          if n._op:
              # if this value is a result of some operation, create an op node for it
              dot.node(name = uid + n._op, label = n._op)
              # and connect this node to it
              dot.edge(uid + n._op, uid)

      for n1, n2 in edges:
          # connect n1 to the op node of n2
          dot.edge(str(id(n1)), str(id(n2)) + n2._op)

      return dot
#+end_src

We previously discussed how to back propagate gradients manually and now we are
going to expand on our =Value= class so that we can more easily automate back
propagation.

We are going to start doing this by reimplementing the =Value= class we had
previous built out in the last section, however this time we are going to expand
its functionality to support a few more [[https://blog.finxter.com/python-list-of-dunder-methods/][dunder]] methods to expand its
capabilities. We are also going to a new property to our init method called
~_backward~. This property will be used to keep track how each of our nodes are
chained together so that we can walk backwards through our nodes. Along with
this each of our operator dunder methods will now have to track which node they
are paired with so we can calculate the gradients.

Finally we are going to define a ~_backward~ function in each of our dunder
methods that will calculate the gradient of each node. Along with this we are
going to define a ~backward~ function in our class that we will be able to call
on our root node to kick off this backwards walkthrough of our equation.

That was a lot to explain but it should all make sense when you read through
it. With all of that explanation out of the way here is our new =Value= class:
#+begin_src jupyter-python :results none
  # This Value class has been expanded upon from gradients section
  class Value:
      def __init__(self, data, _children=(), _op='', label=''):
          self.data = data
          self.grad = 0.0
          self._backward = lambda: None
          self._prev = set(_children)
          self._op = _op
          self.label = label

      def __repr__(self):
          return f"Value(data={self.data})"

      def __add__(self, other):
          other = other if isinstance(other, Value) else Value(other)
          out = Value(self.data + other.data, (self, other), '+')

          def _backward():
              self.grad += 1.0 * out.grad
              other.grad += 1.0 * out.grad
          out._backward = _backward

          return out

      def __mul__(self, other):
          other = other if isinstance(other, Value) else Value(other)
          out = Value(self.data * other.data, (self, other), '*')

          def _backward():
              self.grad += other.data * out.grad
              other.grad += self.data * out.grad
          out._backward = _backward

          return out

      def __pow__(self, other):
          assert isinstance(other, (int, float)), "only supporting int/float powers for now"
          out = Value(self.data**other, (self,), f'**{other}')

          def _backward():
              self.grad += other * (self.data ** (other - 1)) * out.grad
          out._backward = _backward

          return out

      def __radd__(self, other): # other + self
          return self + other

      def __rmul__(self, other): # other * self
          return self * other

      def __rtruediv__(self, other): # other / self
          return other / self.data

      def __rpow__(self, other): # other**self
          return other**self.data

      def __rsub__(self, other): # other - self
          return other - self.data

      def __truediv__(self, other): # self / other
          return self * other**-1

      def __neg__(self): # -self
          return self * -1

      def __sub__(self, other): # self - other
          return self + (-other)

      def tanh(self):
          x = self.data
          t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)
          out = Value(t, (self, ), 'tanh')

          def _backward():
              self.grad += (1 - t**2) * out.grad
          out._backward = _backward

          return out

      def exp(self):
          x = self.data
          out = Value(math.exp(x), (self, ), 'exp')

          def _backward():
              self.grad += out.data * out.grad
          out._backward = _backward

          return out

      # used to kick off our back propagation
      def backward(self):
          # build a topologic graph
          topo = []
          visited = set()
          def build_topo(v):
              if v not in visited:
                  visited.add(v)
                  for child in v._prev:
                      build_topo(child)
                  topo.append(v)
          build_topo(self)

          self.grad = 1.0
          for node in reversed(topo):
              node._backward()

#+end_src

Now let's reimplement the final equation we put together in the last section and
see if we can automatically back propagate the gradients in our equation:
#+begin_src jupyter-python :tangle no :results none
  # inputs x1,x2
  x1 = Value(2.0, label='x1')
  x2 = Value(0.0, label='x2')
  # weights w1,w2
  w1 = Value(-3.0, label='w1')
  w2 = Value(1.0, label='w2')
  # bias of the neuron
  # - This number was chosen to give simpiler numbers to work with during
  #   backpropagation
  b = Value(6.8813735870195432, label='b')
  # x1*w1 + x2*w2 + b
  x1w1 = x1 * w1; x1w1.label = 'x1*w1'
  x2w2 = x2 * w2; x2w2.label = 'x2*w2'
  x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'
  n = x1w1x2w2 + b; n.label = 'n'
  o = n.tanh(); o.label = 'o'
#+end_src

With those redeclared in our new =Value= class we should be able to kick off
back propagation like this:
#+begin_src jupyter-python :tangle no :results none
  # back propagate gradient
  o.backward()
#+end_src

Let's now graph our equation and see if it looks right:
#+name: graph
#+begin_src jupyter-python :tangle no :exports both :file images/graph.svg
  draw_dot(o)
#+end_src

#+RESULTS: graph
:results:
[[file:images/graph.svg]]
:end:

Yes! We have successfully back propagated automatically. Let's also test out
some of the other operations we added to our =Value= class:
#+begin_src jupyter-python :tangle no :results none
  # inputs x1,x2
  x1 = Value(2.0, label='x1')
  x2 = Value(0.0, label='x2')
  # weights w1,w2
  w1 = Value(-3.0, label='w1')
  w2 = Value(1.0, label='w2')
  # bias of the neuron
  # - This number was chosen to give simpiler numbers to work with during
  #   backpropagation
  b = Value(6.8813735870195432, label='b')
  # x1*w1 + x2*w2 + b
  x1w1 = x1 * w1; x1w1.label = 'x1*w1'
  x2w2 = x2 * w2; x2w2.label = 'x2*w2'
  x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'
  n = x1w1x2w2 + b; n.label = 'n'
  # -----
  e = (2*n).exp()
  o = (e - 1) / (e + 1)
  o.label = 'o'
  # -----
#+end_src

And now let's see if we can successfully back propagate this and graph it:
#+name: graph2
#+begin_src jupyter-python :tangle no :exports both :file images/graph2.svg
  o.backward()

  draw_dot(o)
#+end_src

#+RESULTS: graph2
:results:
[[file:images/graph2.svg]]
:end:

At this point our =Value= class implements much of the same logic that exists in
=PyTorch= so let's take a look at how this same equation could be built using
=PyTorch=. Let's begin by import =PyTorch=:
#+begin_src jupyter-python :results none :tangle no
  import torch
#+end_src

And now we can build out our equation using Tensors (in short Tensors are
[[https://www.doitpoms.ac.uk/tlplib/tensors/what_is_tensor.php][multi deminsional arrays]] of scalars). You will also notice that we are setting
~requires_grad~ to true for each of our nodes. This is because by default
PyTorch doesn't keep track of gradient values for leaf nodes. This is done for
efficiency as normally you would not need to track gradients for your input
data. You will also notice that we are casting each of Tensors to ~double~ so
that the Tensors will be in ~float64~ to match our =Value= class above:
#+begin_src jupyter-python :results none :tangle no
  x1 = torch.Tensor([2.0]).double();               x1.requires_grad = True
  x2 = torch.Tensor([0.0]).double();               x2.requires_grad = True
  w1 = torch.Tensor([-3.0]).double();              w1.requires_grad = True
  w2 = torch.Tensor([1.0]).double();               w2.requires_grad = True
  b = torch.Tensor([6.8813735870195432]).double(); b.requires_grad = True

  n = x1*w1 + x2*w2 + b
  o = torch.tanh(n)
#+end_src

Now PyTorch actually has a ~backward~ function just like we defined in our
=Value= class so we can actually call that =o= just like we did above. Also
take note that when we want to print the value of an object when using PyTorch
we use ~.item()~ to print the value. Without this we will simply print out the
tensor data (this does include the value, but it also includes the shape and
data type of the tensor):
#+begin_src jupyter-python :exports both :tangle no
  # Let's print out the value of o to confirm it is the same as our above example
  print(o.data.item())

  # Then let's back propagate our gradients
  o.backward()
#+end_src

#+RESULTS:
:results:
: 0.7071066904050358
:end:

We did in fact get the same value for =o=. Let's also print out the gradients of
our inputs and see if we get the same results as before:
#+begin_src jupyter-python :exports both :tangle no
  print('---')
  print('x2', x2.grad.item())
  print('w2', w2.grad.item())
  print('x1', x1.grad.item())
  print('w1', w1.grad.item())
#+end_src

#+RESULTS:
:results:
: ---
: x2 0.5000001283844369
: w2 0.0
: x1 -1.5000003851533106
: w1 1.0000002567688737
:end:

We did get the same results! This means we have successfully automated back
propagation using our =Value= class and confirmed that it matches the outputs of
PyTorch.

* Implementing micrograd
  So to continue we are going to build out neural network using our =Value=
  class that we created. Before we do that though let's talk about how the math
  we built out actually replicates a neural network ([[https://cs231n.github.io/convolutional-networks/][reference]]).

  A high level view of a neural network could be summarized like this:
  [[file:images/neural_net.jpeg]]

  In short a neural network is a layer of input neurons, =N= number of hidden
  layers of neurons in the middle, and an output layer of neurons. The hidden
  layers do some form of manipulation on the input data to form the output
  layer. Each of these neurons can be visualized like:
  [[file:images/neuron_model.jpeg]]

  Now in a biological world neurons are incredibly complicated and still not
  fully understood, but in the mathematical sense they can be represented by the
  image above. We can describe this model as having some amount of inputs
  (=x='s). These =x= inputs are interacted with synapses (=w='s)
  multiplicatively (=wx=) and passed to the cell body of the neuron. The cell
  body of the neuron has some sort of bias (=b=). This can be thought of as the
  innate trigger happiness of this neuron (meaning it can increase or decrease
  the trigger happiness of this neuron), which is added to the sum of all of our
  =wx= inputs. Finally this is run through some sort of activation function
  =f=. This activation function is usual some type of squashing function like
  for instance a [[https://en.wikipedia.org/wiki/Hyperbolic_functions#Definitions][tanh]] or other hyperbolic function.

  The output of our neuron then could be written in the mathematical expression:
  [[file:images/neuron_equation.svg]]

  So now let's look at how we can actually use our =Value= class to build out a
  multi layer perceptron (our case probably a two layer perceptron). Let's begin
  by defining a class for Neurons that use our =Value= class. Our =Neuron=
  class will take =n= number of inputs so for our ~__init__~ method will take
  in a variable called =nin=. We will also make user of the ~__call__~ method
  to implement the function we shown above:
  #+begin_src jupyter-python :results none
    import random

    class Neuron:
        # nin = Number of inputs
        def __init__(self, nin):
            self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]
            self.b = Value(random.uniform(-1,1))

        def __call__(self, x):
            # w * x + b
            # The zip function here will pair up our w's with our x's
            act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)
            out = act.tanh()
            return out

        def parameters(self):
            return self.w + [self.b]
  #+end_src

  To test this let's create a neuron with 2 inputs:
  #+name: nueron
  #+begin_src jupyter-python :exports both
    x = [2.0, 3.0]
    n = Neuron(2)

    n(x)
  #+end_src

  Every time we create a neuron will get a random weight and bias value so we
  will get a different output. This time we got:
  #+RESULTS: nueron
  :results:
  : Value(data=0.33548597941008335)
  :end:

  The next layer of abstraction in our diagram of neural networks is that we
  need to have layers of neurons. When we have a layer of neurons the neurons
  themselves in the layer are not necessarily connected to each other, but each
  neuron in the layer is connected to the neurons in the previous layer. To
  represent this in our code we are going to create a =Layer= class. Our
  =Layer= class is going to take in a =n= number of input neurons and =n=
  number of output neurons (nin and nout) and will keep track of it's own
  output neurons so that we can chain layers together (outs):
  #+begin_src jupyter-python :results none
    # Let's create a layer of Neurons
    class Layer:
        # nout = Number of output Neurons
        def __init__(self, nin, nout):
            self.neurons = [Neuron(nin) for _ in range(nout)]

        def __call__(self, x):
            # outs = Number of neurons in this layer
            outs = [n(x) for n in self.neurons]
            return outs[0] if len(outs) == 1 else outs

        def parameters(self):
            return [p for neuron in self.neurons for p in neuron.parameters()]
  #+end_src

  So now we can create a layer of neurons:
  #+name: layer
  #+begin_src jupyter-python :exports both
    x = [2.0, 3.0]
    # A layer with 2 inputs and 3 outputs
    n = Layer(2, 3)

    n(x)
  #+end_src

  Again every time we create a layer we are going to get random weights and
  biases on our neurons, but this time we got the following:
  #+RESULTS: layer
  :results:
  | Value | (data=-0.9400793949715526) | Value | (data=0.9140586663774759) | Value | (data=0.9615660989674627) |
  :end:

  It is also worth noting that in our =Layer= class we defined an if statement
  on our return where if we are looking at the last layer we can just return our
  0th element of our =outs= list as it is the only element. Without this
  conditional our print statement here would be in a list when we don't need it
  to be.

  To complete the neural network diagram we are going to define a multi layer
  perceptron which will define in an =MLP= class. Our =MLP= class will take =n=
  number of inputs and this time will take list of =n= number of outputs, which
  define the sizes of all of the layers in our MLP:
  #+begin_src jupyter-python :results none
    # Let's create an MLP (Multi Layer Perceptron)
    class MLP:
        # nouts = list of nout
        def __init__(self, nin, nouts):
            sz = [nin] + nouts
            self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]

        def __call__(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

        def parameters(self):
            return [p for layer in self.layers for p in layer.parameters()]
  #+end_src

  Now let's create an MLP using our =MLP= class:
  #+name: mlp
  #+begin_src jupyter-python :exports both
    x = [2.0, 3.0, -1.0]
    # 3 inputs into 2 layers of 4 and 1 output
    n = MLP(3, [4, 4, 1])

    n(x)
  #+end_src

  With this we get the result of a forward pass of our MLP, this time we got:
  #+RESULTS: mlp
  :results:
  : Value(data=-0.790206267391274)
  :end:

  At this point we can actually run ~draw_dot~ on our MLP to get a graph of our
  entire network:
  #+name: mlpgraph
  #+begin_src jupyter-python :exports both :file images/mlpgraph.svg
    draw_dot(n(x))
  #+end_src

  #+RESULTS: mlpgraph
  :results:
  [[file:images/mlpgraph.svg]]
  :end:

# Local Variables:
# org-image-actual-width: (1024)
# End:
